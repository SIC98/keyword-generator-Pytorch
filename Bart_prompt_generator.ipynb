{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d78e768-74a2-4d25-a47f-21e247aa09c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"51la5/keyword-extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "96a4b53b-16a8-4634-a0dd-275aabeb07c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['dataset', 'file_id', 'text', 'summary', 'type'],\n",
       "        num_rows: 22033\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['dataset', 'file_id', 'text', 'summary', 'type'],\n",
       "        num_rows: 5513\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a66e4337-430d-4fc0-b894-980600e24d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Inspec',\n",
       " 'Krapivin2009',\n",
       " 'Nguyen2007',\n",
       " None,\n",
       " 'PubMed',\n",
       " 'QMSum',\n",
       " 'Schutz2008',\n",
       " 'SemEval2010',\n",
       " 'SemEval2017',\n",
       " 'citeulike180',\n",
       " 'fao30',\n",
       " 'fao780',\n",
       " 'kdd',\n",
       " 'theses100',\n",
       " 'wiki20',\n",
       " 'www'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dataset['train'][\"dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b9efe148-86a7-4b35-803e-7733b3b9a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda example: example[\"type\"] == \"KEYWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9f13620b-de2b-4ab9-80d5-a5fd1973199a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['dataset', 'file_id', 'text', 'summary', 'type'],\n",
       "        num_rows: 8139\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['dataset', 'file_id', 'text', 'summary', 'type'],\n",
       "        num_rows: 2033\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aa432453-b152-4eb4-bd9b-9bbb57d1e286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8139/8139 [00:00<00:00, 45757.15 examples/s]\n",
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2033/2033 [00:00<00:00, 51768.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda example: len(example[\"text\"]) <= 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4598be4-a32c-429b-ad1d-538718694a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['dataset', 'file_id', 'text', 'summary', 'type'],\n",
       "        num_rows: 2522\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['dataset', 'file_id', 'text', 'summary', 'type'],\n",
       "        num_rows: 630\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab1a1e1c-5f17-4594-ae8f-6f21b7893bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2522/2522 [00:00<00:00, 105500.77 examples/s]\n",
      "Filter: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 630/630 [00:00<00:00, 96572.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda example: len(example[\"text\"]) / len(example[\"summary\"]) <= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "104074f1-7e31-4a68-bb72-80094cd19e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['dataset', 'file_id', 'text', 'summary', 'type'],\n",
       "        num_rows: 2319\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['dataset', 'file_id', 'text', 'summary', 'type'],\n",
       "        num_rows: 571\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e16fc921-8bbc-4e1a-aa51-3b9e1cdc7d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "from tabulate import tabulate\n",
    "import nltk\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "157439ba-2b79-4a02-b522-5c13e52d631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_INTEGRATION = True\n",
    "if WANDB_INTEGRATION:\n",
    "    import wandb\n",
    "\n",
    "    wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1aa1771d-32ae-4486-84e4-c8b8e498f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-base\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f36e031a-e389-4b40-91b1-d3f4200ac168",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_max_length = 512  # demo\n",
    "decoder_max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ff34bfea-c381-44a0-9fc1-3c3bb6e95633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'kdd',\n",
       " 'file_id': '1370349',\n",
       " 'text': 'Mining a stream of transactions for customer patterns No contact information provided yet.',\n",
       " 'summary': 'approximate queries,customer profiles,dynamic database,histograms,incremental updates,massive data,signatures',\n",
       " 'type': 'KEYWORD'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7dde81ad-2c19-4b62-b38b-bb2126a2b9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2319/2319 [00:00<00:00, 2817.54 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 571/571 [00:00<00:00, 2918.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def batch_tokenize_preprocess(batch, tokenizer, max_source_length, max_target_length):\n",
    "    source_tokenized = tokenizer(\n",
    "        batch[\"summary\"], padding=\"max_length\", truncation=True, max_length=max_source_length\n",
    "    )\n",
    "    target_tokenized = tokenizer(\n",
    "        batch[\"text\"], padding=\"max_length\", truncation=True, max_length=max_target_length\n",
    "    )\n",
    "\n",
    "    batch = {k: v for k, v in source_tokenized.items()}\n",
    "    # Ignore padding in the loss\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in l]\n",
    "        for l in target_tokenized[\"input_ids\"]\n",
    "    ]\n",
    "    return batch\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda batch: batch_tokenize_preprocess(\n",
    "        batch, tokenizer, encoder_max_length, decoder_max_length\n",
    "    ),\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "becee1ef-ed57-45de-a371-45f98fba3f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'kdd',\n",
       " 'file_id': '1370349',\n",
       " 'text': 'Mining a stream of transactions for customer patterns No contact information provided yet.',\n",
       " 'summary': 'approximate queries,customer profiles,dynamic database,histograms,incremental updates,massive data,signatures',\n",
       " 'type': 'KEYWORD'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4d6ac5e1-56e1-48ff-8548-8716ff8b13e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3026/2143885136.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = datasets.load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "# !pip install rouge-score\n",
    "metric = datasets.load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "56780a96-825c-4900-8530-1a05dbb70659",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "metric = datasets.load_metric(\"rouge\")\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract a few results from ROUGE\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d2567240-ca8c-4587-934e-c73b27e12ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    num_train_epochs=20,  # demo\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16, # demo\n",
    "    per_device_eval_batch_size=16,\n",
    "    # learning_rate=3e-05,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.1,\n",
    "    label_smoothing_factor=0.1,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eaf7b3bd-4c09-4d86-8c7a-8313d1a93b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sic/Documents/projects/kakao-keyword-to-prompt-generator/wandb/run-20230729_111630-5s6nasm0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/buffett/bart%20prompt%20generator/runs/5s6nasm0' target=\"_blank\">copper-haze-5</a></strong> to <a href='https://wandb.ai/buffett/bart%20prompt%20generator' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/buffett/bart%20prompt%20generator' target=\"_blank\">https://wandb.ai/buffett/bart%20prompt%20generator</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/buffett/bart%20prompt%20generator/runs/5s6nasm0' target=\"_blank\">https://wandb.ai/buffett/bart%20prompt%20generator/runs/5s6nasm0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_run = wandb.init(\n",
    "    project=\"bart prompt generator\",\n",
    "    config={\n",
    "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"dataset\": \"51la5/keyword-extraction\",\n",
    "    },\n",
    ")\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H%M%S\")\n",
    "wandb_run.name = \"run_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8b3ee52f-4878-45f2-a297-862fcec3709d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='253' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 1:00:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 7.8832197189331055,\n",
       " 'eval_rouge1': 20.3832,\n",
       " 'eval_rouge2': 8.3047,\n",
       " 'eval_rougeL': 16.7813,\n",
       " 'eval_rougeLsum': 17.2561,\n",
       " 'eval_gen_len': 18.6865,\n",
       " 'eval_runtime': 18.6698,\n",
       " 'eval_samples_per_second': 30.584,\n",
       " 'eval_steps_per_second': 1.928}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "59d4dae1-de6a-4dfa-83a5-add8828d6e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sic/Documents/projects/kakao-keyword-to-prompt-generator/venv/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2900' max='2900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2900/2900 26:59, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.341600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.329000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.927600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.841100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.764800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.657000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.604400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>4.566800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.418200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>4.427900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.368300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.258100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.241500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.106500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>4.088300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.945700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.949600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.964700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>3.864200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.812600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>3.811600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.735800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>3.704200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.716500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>3.602800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.618800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>3.627800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.514800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>3.519900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.534200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>3.429200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>3.451300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>3.432500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.346500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>3.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>3.360700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>3.336700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.308700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>3.299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>3.272900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>3.245700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.232300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>3.216400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>3.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>3.168600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.163700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>3.203500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.136200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>3.142600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>3.146900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>3.123000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>3.127700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>3.135700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>3.086500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>3.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>3.114200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2900, training_loss=3.7804891020676186, metrics={'train_runtime': 1620.3423, 'train_samples_per_second': 28.624, 'train_steps_per_second': 1.79, 'total_flos': 1.41397884665856e+16, 'train_loss': 3.7804891020676186, 'epoch': 20.0})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7a8f2dd7-2903-48ea-8695-d67b4c97177b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.468584060668945,\n",
       " 'eval_rouge1': 33.2493,\n",
       " 'eval_rouge2': 21.6726,\n",
       " 'eval_rougeL': 30.3571,\n",
       " 'eval_rougeLsum': 31.135,\n",
       " 'eval_gen_len': 18.4221,\n",
       " 'eval_runtime': 18.5853,\n",
       " 'eval_samples_per_second': 30.723,\n",
       " 'eval_steps_per_second': 1.937,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "562efc6a-2861-483c-9063-fca3b1648813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.7313904762268066,\n",
       " 'eval_rouge1': 34.7148,\n",
       " 'eval_rouge2': 24.2124,\n",
       " 'eval_rougeL': 32.4001,\n",
       " 'eval_rougeLsum': 33.0395,\n",
       " 'eval_gen_len': 18.5054,\n",
       " 'eval_runtime': 73.9681,\n",
       " 'eval_samples_per_second': 31.351,\n",
       " 'eval_steps_per_second': 1.96,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8dbf0224-86ec-4743-a831-5877bee0f717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cat and mouse: getting the cat and the dog dirty No contact information provided yet.']\n"
     ]
    }
   ],
   "source": [
    "keywords = \"cat, clean, dog, dirty\"\n",
    "inputs = tokenizer(\n",
    "    keywords,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=encoder_max_length,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "input_ids = inputs.input_ids.to(model.device)\n",
    "attention_mask = inputs.attention_mask.to(model.device)\n",
    "outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4ed7497b-8d4d-4b23-b0d4-2d68f7403584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(test_samples, model):\n",
    "    inputs = tokenizer(\n",
    "        test_samples[\"summary\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=encoder_max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(model.device)\n",
    "    attention_mask = inputs.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs, output_str\n",
    "\n",
    "\n",
    "model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "test_samples = dataset[\"test\"].select(range(16))\n",
    "\n",
    "summaries_before_tuning = generate_summary(test_samples, model_before_tuning)[1]\n",
    "summaries_after_tuning = generate_summary(test_samples, model)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f761dd45-a826-41cf-bfd1-5bae73a8b1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'Inspec',\n",
       " 'file_id': '109',\n",
       " 'text': 'An entanglement measure based on the capacity of dense codingAn asymptotic entanglement measure for any bipartite states is derived in thelight of the dense coding capacity optimized with respect to localquantum operations and classical communications. General properties andsome examples with explicit forms of this entanglement measure areinvestigated',\n",
       " 'summary': 'entanglement measure,dense coding capacity,asymptotic entanglement measure,bipartite states,local quantum operations,classical communications,optimization,encoding,optimisation,quantum communication',\n",
       " 'type': 'KEYWORD'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5abfa612-2961-4b52-ac1e-e83a95cb5b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Id  Generated Prompt                                                                                                        Keywords\n",
      "----  ----------------------------------------------------------------------------------------------------------------------  ---------------------------------------------------------------------------------------------------\n",
      "   0  An asymptotic entanglement measure for quantum operations with a dense codingcapacity                                   entanglement measure,dense coding capacity,asymptotic entanglement\n",
      "   1  Geographic location of servers in africa: a digital-divide approach No contact                                          africa,cctld,digital-divide,geographic location of servers\n",
      "   2  A voltage-vector selection algorithm for direct torque control of induction motordrivesA new                            voltage-vector selection algorithm,direct torque control,induction motor iphone\n",
      "   3  An agent communication languages for the semantic web No contact information provided yet.                              agent communication languages,daml-s,ontologies,semantic web,web\n",
      "   4  Similarity detection in document management No contact information provided yet.                                        content analysis and indexing,content management,document management,near duplicate detection,sc\n",
      "   5  In this paper we construct the most general solutions with a four-dimensional Poincar                                   3-branes with negative tension,axial symmetry in the two-dimensional internal\n",
      "   6  Sensorless control of induction motor drives with stability andstator voltages andst                                    sensorless control,induction motor drives,reliability,stator voltages\n",
      "   7  Integration of e-commerce transactions witheries: an application to information integration and retrieval               e-commerce transactions,queries,information integration,information Âretrieval\n",
      "   8  Mixture model-based learning No contact information provided yet.                                                       alignment,curve clustering,em,learning,mixture model,trans\n",
      "   9  Fractional motion control of theXY cutting tableA method for path tracking design is                                    fractional motion control,XY cutting table,path tracking design,actuators\n",
      "  10  Block importance model for web segmentation No contact information provided yet.                                        block importance model,classification,miscellaneous,page segmentation,web mining\n",
      "  11  Information architecture for the target userThis paper presents an information architecture for designing andimplement  information architecture,target user,target content,children's Web portal,museum information\n",
      "  12  Techniques and changes in the electric power industry deregulationThe author looks at some of the                       electric power industry deregulation,computer applications,electricity ills,industry restructuring,\n",
      "  13  Adaptive hypermedia for computer uses in education No contact information provided yet.                                 adaptive hypermedia,computer uses in education,content-based adaptation mechanism,distance\n",
      "  14  Group and organization interfaces for web applications No contact information provided yet.                             group and organization interfaces\n",
      "  15  In this paper, we propose an objective analysis on the sports-biomechanics                                              data and essays,deep development and training of walking race,discussing and analyzing\n",
      "\n",
      "Target text:\n",
      "\n",
      "  Id  Target text\n",
      "----  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "   0  An entanglement measure based on the capacity of dense codingAn asymptotic entanglement measure for any bipartite states is derived in thelight of the dense coding capacity optimized with respect to localquantum operations and classical communications. General properties andsome examples with explicit forms of this entanglement measure areinvestigated\n",
      "   1  Geographic locations of web servers No contact information provided yet.\n",
      "   2  A new voltage-vector selection algorithm in direct torque control of inductionmotor drivesAC drives based on direct torque control of induction machines allow highdynamic performance to be obtained with very simple control schemes.The drive behavior, in terms of current, flux and torque ripple, isdependent on the utilised voltage vector selection strategy and theoperating conditions. In this paper a new voltage vector selectionalgorithm, which allows a sensible reduction of the RMS value of thestator current ripple without increasing the average value of theinverter switching frequency and without the need of a PWM pulsegenerator block is presented Numerical simulations have been carriedout to validate the proposed method\n",
      "   3  Agent-based semantic web services No contact information provided yet.\n",
      "   4  Finding similar files in large document repositories No contact information provided yet.\n",
      "   5  We prove the uniqueness of the supersymmetric Salam–Sezgin (Minkowski)4×S2 ground state among all non-singular solutions with a four-dimensional Poincaré, de Sitter or anti-de Sitter symmetry. We construct the most general solutions with an axial symmetry in the two-dimensional internal space, and show that included amongst these is a family that is non-singular away from a conical defect at one pole of a distorted 2-sphere. These solutions admit the interpretation of 3-branes with negative tension.\n",
      "   6  Sensorless control of induction motor drivesControlled induction motor drives without mechanical speed sensors at the motorshaft have the attractions of low cost and high reliability. To replacethe sensor the information on the rotor speed is extracted frommeasured stator voltages and currents at the motor terminals.Vector-controlled drives require estimating the magnitude and spatialorientation of the fundamental magnetic flux waves in the stator or inthe rotor. Open-loop estimators or closed-loop observers are used forthis purpose. They differ with respect to accuracy, robustness, andsensitivity against model parameter variations. Dynamic performance andsteady-state speed accuracy in the low-speed range can be achieved byexploiting parasitic effects of the machine. The overview in this paperuses signal flow graphs of complex space vector quantities to providean insightful description of the systems used in sensorless control ofinduction motors\n",
      "   7  Semantic B2B integration: issues in ontology-based approachesSolving queries to support e-commerce transactions can involve retrieving andintegrating information from multiple information resources. Often,users don't care which resources are used to answer their query. Insuch situations, the ideal solution would be to hide from the user thedetails of the resources involved in solving a particular query. Anexample would be providing seamless access to a set of heterogeneouselectronic product catalogues. There are many problems that must beaddressed before such a solution can be provided. In this paper, wediscuss a number of these problems, indicate how we have addressedthese and go on to describe the proof-of-concept demonstration systemwe have developed\n",
      "   8  Translation-invariant mixture models for curve clustering No contact information provided yet.\n",
      "   9  Fractional motion control: application to an XY cutting tableIn path tracking design, the dynamic of actuators must be taken into account inorder to reduce overshoots appearing for small displacements. A newapproach to path tracking using fractional differentiation is proposedwith its application on a XY cutting table. It permits the generationof optimal movement reference-input leading to a minimum pathcompletion time, taking into account both maximum velocity,acceleration and torque and the bandwidth of the closed-loop system.Fractional differentiation is used here through a Davidson-Cole filter.A methodology aiming at improving the accuracy especially oncheckpoints is presented. The reference-input obtained is compared withspline function. Both are applied to an XY cutting table model andactuator outputs compared\n",
      "  10  Learning block importance models for web pages No contact information provided yet.\n",
      "  11  Information architecture for the Web: The IA matrix approach to designingchildren's portalsThe article presents a matrix that can serve as a tool for designing theinformation architecture of a Web portal in a logical and systematicmanner. The information architect begins by inputting the portal'sobjective, target user, and target content. The matrix then determinesthe most appropriate information architecture attributes for the portalby filling in the Applied Information Architecture portion of thematrix. The article discusses how the matrix works using the example ofa children's Web portal to provide access to museum information\n",
      "  12  Prospective on computer applications in powerThe so-called \"deregulation\" and restructuring of the electric power industryhave made it very difficult to keep up with industry changes and havemade it much more difficult to envision the future. In this article,current key issues and major developments of the past few years arereviewed to provide perspective, and prospects for future computerapplications in power are suggested. Technology changes are occurringat an exponential rate. The interconnected bulk electric systems arebecoming integrated with vast networked information systems. Thisarticle discusses the skills that will be needed by future powerengineers to keep pace with these developments and trends\n",
      "  13  Fine grained content-based adaptation mechanism for providing high end-user quality of experience with adaptive hypermedia systems No contact information provided yet.\n",
      "  14  Item-based collaborative filtering recommendation algorithms An abstract is not available.\n",
      "  15  By referring to many relevant data and essays, this paper aims at discussing and analyzing the importance of hip-push applied in walking race,based on the point of the view on sports biomechanics .With redard to the existing problems,the authors have made an objective analysis on the sports-biomechanics factors that can influence the race,hoping to provide a theoretical basis for the deep development and training of walking race.\n",
      "\n",
      "Source documents:\n",
      "\n",
      "  Id  summary\n",
      "----  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "   0  entanglement measure,dense coding capacity,asymptotic entanglement measure,bipartite states,local quantum operations,classical communications,optimization,encoding,optimisation,quantum communication\n",
      "   1  africa,cctld,digital-divide,geographic location of servers,nic registration information,number of hops,offshore server,response time,traceroute\n",
      "   2  voltage-vector selection algorithm,direct torque control,induction motor \tdrives,AC drives,high dynamic performance,torque ripple,voltage \tvector selection strategy,operating conditions,RMS value,stator \tcurrent ripple,inverter switching frequency,torque variations,flux \tvariations,4-poles induction motor,steady-state operation,dynamic \tbehavior,torque step response,220 V,50 Hz,4 kW,induction motor drives,invertors,machine control,stators,torque control\n",
      "   3  agent communication languages,daml-s,ontologies,semantic web,web services\n",
      "   4  content analysis and indexing,content management,document management,near duplicate detection,scalability,similarity\n",
      "   5  3-branes with negative tension,axial symmetry in the two-dimensional internal space,,construct the most general solutions,de Sitter or anti-de Sitter symmetry,interpretation of 3-branes with negative tension,non-singular away from a conical defect,non-singular solutions with a four-dimensional Poincaré,prove the uniqueness\n",
      "   6  sensorless control,induction motor drives,reliability,stator voltages,stator currents,vector-controlled drives,magnitude,spatial \torientation,fundamental magnetic flux waves,open-loop estimators,closed-loop observers,robustness,sensitivity,model parameter \tvariations,steady-state speed accuracy,parasitic effects,signal flow \tgraphs,space vector quantities,closed loop systems,induction motor drives,reliability,robust control,signal flow graphs,stators\n",
      "   7  e-commerce transactions,queries,information integration,information \tretrieval,multiple information resources,heterogeneous electronic \tproduct catalogues,ontology-based approaches,semantic B2B integration,client-server systems,distributed databases,electronic commerce,knowledge \tengineering,nomenclature,query processing\n",
      "   8  alignment,curve clustering,em,learning,mixture model,transformation invariance\n",
      "   9  fractional motion control,XY cutting table,path tracking design,actuators,fractional differentiation,minimum path completion time,closed-loop \tsystem,Davidson-Cole filter,spline function,optimization,calculus,cutting,motion control,optimisation,polynomial approximation,position control,splines (mathematics),tracking filters\n",
      "  10  block importance model,classification,miscellaneous,page segmentation,web mining\n",
      "  11  information architecture,target user,target content,children's Web portal,museum information,computer aided instruction,electronic publishing,exhibitions,humanities,information resources\n",
      "  12  electric power industry deregulation,computer applications,electricity \tindustry restructuring,technology changes,interconnected bulk \telectric systems,networked information systems,electricity supply industry,management information systems,power engineering \tcomputing\n",
      "  13  adaptive hypermedia,computer uses in education,content-based adaptation mechanism,distance education,end-user quality of experience\n",
      "  14  group and organization interfaces\n",
      "  15  data and essays,deep development and training of walking race,discussing and analyzing the importance of hip-push applied in walking race,hip-push,objective analysis on the sports-biomechanics factors,sports biomechanics,theoretical basis\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tabulate(\n",
    "        zip(\n",
    "            range(len(summaries_after_tuning)),\n",
    "            summaries_after_tuning,\n",
    "            summaries_before_tuning,\n",
    "        ),\n",
    "        headers=[\"Id\", \"Generated Prompt\", \"Keywords\"],\n",
    "    )\n",
    ")\n",
    "print(\"\\nTarget text:\\n\")\n",
    "print(\n",
    "    tabulate(list(enumerate(test_samples[\"text\"])), headers=[\"Id\", \"Target text\"])\n",
    ")\n",
    "print(\"\\nSource documents:\\n\")\n",
    "print(tabulate(list(enumerate(test_samples[\"summary\"])), headers=[\"Id\", \"summary\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fcfb7e-aa2d-4573-acaf-c7bddd66388b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94cb4af-e8e5-4dfd-a460-78a40a245b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
